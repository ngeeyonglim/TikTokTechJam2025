{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08ET9yIVopk",
        "outputId": "f4aa8511-e58e-4539-e6fb-f691ac119ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in ./.venv/lib/python3.13/site-packages (8.3.189)\n",
            "Requirement already satisfied: flwr in ./.venv/lib/python3.13/site-packages (1.20.0)\n",
            "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.13/site-packages (0.34.4)\n",
            "Requirement already satisfied: opencv-python in ./.venv/lib/python3.13/site-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (11.3.0)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (0.23.0)\n",
            "Requirement already satisfied: opacus in ./.venv/lib/python3.13/site-packages (1.5.4)\n",
            "Requirement already satisfied: imageio in ./.venv/lib/python3.13/site-packages (2.37.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in ./.venv/lib/python3.13/site-packages (from ultralytics) (2.2.6)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in ./.venv/lib/python3.13/site-packages (from ultralytics) (3.10.5)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in ./.venv/lib/python3.13/site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in ./.venv/lib/python3.13/site-packages (from ultralytics) (2.32.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.13/site-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in ./.venv/lib/python3.13/site-packages (from ultralytics) (2.8.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from ultralytics) (7.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in ./.venv/lib/python3.13/site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in ./.venv/lib/python3.13/site-packages (from ultralytics) (1.32.3)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./.venv/lib/python3.13/site-packages (from ultralytics) (2.0.16)\n",
            "Requirement already satisfied: click<8.2.0 in ./.venv/lib/python3.13/site-packages (from flwr) (8.1.8)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in ./.venv/lib/python3.13/site-packages (from flwr) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in ./.venv/lib/python3.13/site-packages (from flwr) (1.74.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.62.3 in ./.venv/lib/python3.13/site-packages (from flwr) (1.62.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in ./.venv/lib/python3.13/site-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in ./.venv/lib/python3.13/site-packages (from flwr) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in ./.venv/lib/python3.13/site-packages (from flwr) (4.25.8)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in ./.venv/lib/python3.13/site-packages (from flwr) (3.23.0)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in ./.venv/lib/python3.13/site-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in ./.venv/lib/python3.13/site-packages (from flwr) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from flwr) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in ./.venv/lib/python3.13/site-packages (from flwr) (0.12.5)\n",
            "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.13/site-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.13/site-packages (from rich<14.0.0,>=13.5.0->flwr) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from typer<0.13.0,>=0.12.5->flwr) (4.15.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.13/site-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in ./.venv/lib/python3.13/site-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: pycparser in ./.venv/lib/python3.13/site-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install ultralytics flwr huggingface_hub opencv-python pillow torchvision opacus imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "YOLO81_NAMES = [\n",
        "    \"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\n",
        "    \"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\n",
        "    \"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\n",
        "    \"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\n",
        "    \"surfboard\",\"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\n",
        "    \"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\n",
        "    \"couch\",\"potted plant\",\"bed\",\"dining table\",\"toilet\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\n",
        "    \"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\"vase\",\n",
        "    \"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\",\"face\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR1FK8dVWrOr",
        "outputId": "fde33d5b-018b-47f8-c698-be3c6673d918"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import imageio.v3 as iio  # noqa: F401\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "chRQDfBxWzVU"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Utils\n",
        "# -----------------------------\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def copy_state_dict(sd: Dict[str, torch.Tensor], device: torch.device) -> Dict[str, torch.Tensor]:\n",
        "    return {k: v.detach().clone().to(device) for k, v in sd.items()}\n",
        "\n",
        "\n",
        "def state_dict_difference(new: Dict[str, torch.Tensor],\n",
        "                          old: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"new - old, for float tensors only; zeros for non-float/buffers.\"\"\"\n",
        "    out = {}\n",
        "    for k, vold in old.items():\n",
        "        vnew = new[k]\n",
        "        if torch.is_floating_point(vold):\n",
        "            out[k] = (vnew.to(vold.dtype) - vold).to(vold.device)\n",
        "        else:\n",
        "            out[k] = torch.zeros_like(vold, device=vold.device)\n",
        "    return out\n",
        "\n",
        "\n",
        "def state_dict_add(base: Dict[str, torch.Tensor],\n",
        "                   delta: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"base + delta for floats; identity for non-floats.\"\"\"\n",
        "    out = {}\n",
        "    for k, vb in base.items():\n",
        "        vd = delta[k]\n",
        "        if torch.is_floating_point(vb):\n",
        "            out[k] = (vb + vd.to(vb.dtype)).to(vb.device)\n",
        "        else:\n",
        "            out[k] = vb\n",
        "    return out\n",
        "\n",
        "\n",
        "def average_state_dicts(sds: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Mean over float tensors across clients; zeros for non-floats.\"\"\"\n",
        "    out = {}\n",
        "    keys = sds[0].keys()\n",
        "    for k in keys:\n",
        "        t0 = sds[0][k]\n",
        "        if torch.is_floating_point(t0):\n",
        "            stacked = torch.stack([sd[k].to(t0.dtype) for sd in sds], dim=0)\n",
        "            out[k] = stacked.mean(dim=0).to(t0.dtype)\n",
        "        else:\n",
        "            out[k] = torch.zeros_like(t0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def l2_norm_of_delta(delta: Dict[str, torch.Tensor]) -> float:\n",
        "    sq = 0.0\n",
        "    for v in delta.values():\n",
        "        sq += float(torch.sum(v.float() ** 2))\n",
        "    return float(np.sqrt(sq))\n",
        "\n",
        "\n",
        "def clip_and_add_noise(delta: Dict[str, torch.Tensor], clip_norm: float, noise_multiplier: float) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Client-level DP: clip the update to L2<=clip_norm, then add N(0, (sigma*clip_norm)^2).\"\"\"\n",
        "    norm = l2_norm_of_delta(delta) + 1e-12\n",
        "    scale = min(1.0, clip_norm / norm)\n",
        "    sigma = noise_multiplier * clip_norm\n",
        "    noised = {}\n",
        "    for k, v in delta.items():\n",
        "        clipped = v * scale\n",
        "        if sigma > 0.0:\n",
        "            noised[k] = clipped + torch.randn_like(clipped) * sigma\n",
        "        else:\n",
        "            noised[k] = clipped\n",
        "    return noised\n",
        "\n",
        "\n",
        "def load_yolo_for_pretrained(weights_path: str, device: torch.device):\n",
        "    \"\"\"\n",
        "    Load an Ultralytics YOLO model from an official checkpoint name (e.g., 'yolov8n.pt')\n",
        "    or a local .pt file. Do NOT force nc/names; Ultralytics' Trainer will auto-adjust\n",
        "    the detection head to match data.yaml (nc) during .train().\n",
        "    \"\"\"\n",
        "    model = YOLO(weights_path)  # auto-downloads by name if needed\n",
        "    model.model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def ensure_weights(weights_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Return the provided weights if they exist, otherwise return an official model name\n",
        "    so Ultralytics auto-downloads (e.g. 'yolov8n.pt').\n",
        "    \"\"\"\n",
        "    if weights_path and Path(weights_path).exists():\n",
        "        return weights_path\n",
        "    # Default to official YOLOv8n if a path isn't provided/found\n",
        "    print(\"No local weights found; using official 'yolov8n.pt' (auto-download).\")\n",
        "    return \"yolov8n.pt\"\n",
        "\n",
        "def align_model_head_to_yaml(model: \"YOLO\", data_yaml: str, device: torch.device, imgsz: int = 640):\n",
        "    \"\"\"\n",
        "    Use Ultralytics' Trainer pipeline to rebuild the detection head so that 'nc'\n",
        "    matches the given data.yaml. epochs=0 avoids actual training but performs\n",
        "    model/data setup.\n",
        "    \"\"\"\n",
        "    _ = model.train(\n",
        "        data=data_yaml,\n",
        "        epochs=0,            # <- no training; just builds according to data.yaml\n",
        "        imgsz=imgsz,\n",
        "        device=0 if device.type == \"cuda\" else \"cpu\",\n",
        "        verbose=False,\n",
        "    )\n",
        "    model.model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "tf6etF_NXfK9"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Federated Client\n",
        "# -----------------------------\n",
        "class FLClient:\n",
        "    def __init__(\n",
        "        self,\n",
        "        client_id: int,\n",
        "        device: torch.device,\n",
        "        weights_path: str,\n",
        "        data_yaml: str,\n",
        "        workdir: Path,\n",
        "        imgsz: int = 640,\n",
        "        local_epochs: int = 1,\n",
        "        batch: int = 16,\n",
        "        dp_enabled: bool = False,\n",
        "        dp_clip: float = 1.0,\n",
        "        dp_noise_mult: float = 0.5,\n",
        "        freeze_backbone: bool = False,\n",
        "    ):\n",
        "        self.client_id = client_id\n",
        "        self.device = device\n",
        "        self.weights_path = weights_path\n",
        "        self.imgsz = imgsz\n",
        "        self.local_epochs = local_epochs\n",
        "        self.batch = batch\n",
        "        self.dp_enabled = dp_enabled\n",
        "        self.dp_clip = dp_clip\n",
        "        self.dp_noise_mult = dp_noise_mult\n",
        "        self.data_yaml = data_yaml\n",
        "        self.freeze_backbone = freeze_backbone\n",
        "\n",
        "        # Local YOLO instance per client (isolated optimizer/EMA/augment state)\n",
        "        self.model = load_yolo_for_pretrained(weights_path, device)\n",
        "        self.run_dir = workdir / f\"client_{client_id}\"\n",
        "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\n",
        "            \"client\", self.client_id,\n",
        "            \"task=\", getattr(self.model.model, \"task\", None),\n",
        "            \"nc=\", getattr(self.model.model, \"nc\", None),\n",
        "            \"names_len=\", len(getattr(self.model.model, \"names\", []))\n",
        "        )\n",
        "\n",
        "    def _freeze_some_layers(self):\n",
        "        if not self.freeze_backbone:\n",
        "            return\n",
        "        # Heuristic: freeze early backbone blocks (YOLOv8n indexing)\n",
        "        for name, param in self.model.model.named_parameters():\n",
        "            if any(name.startswith(f\"model.{i}.\") for i in range(10)):\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def get_state_dict(self) -> Dict[str, torch.Tensor]:\n",
        "        return copy_state_dict(self.model.model.state_dict(), self.device)\n",
        "\n",
        "    def set_state_dict(self, sd: Dict[str, torch.Tensor]):\n",
        "        # HEADS may differ across clients; be tolerant\n",
        "        self.model.model.load_state_dict(sd, strict=False)\n",
        "\n",
        "    def local_train_and_get_update(self, global_sd: Dict[str, torch.Tensor]) -> Tuple[Dict[str, torch.Tensor], dict]:\n",
        "        # Load broadcaster's global params\n",
        "        self.set_state_dict(global_sd)\n",
        "\n",
        "        # Optional freezing\n",
        "        self._freeze_some_layers()\n",
        "\n",
        "        self.model.overrides = getattr(self.model, \"overrides\", {}) or {}\n",
        "        self.model.overrides[\"epochs\"] = int(self.local_epochs)\n",
        "        self.model.overrides[\"resume\"] = False\n",
        "        # Train locally (Ultralytics will adjust head to self.data_yaml's nc)\n",
        "        results = self.model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=self.local_epochs,\n",
        "            imgsz=self.imgsz,\n",
        "            batch=self.batch,\n",
        "            device=0 if self.device.type == \"cuda\" else \"cpu\",\n",
        "            project=str(self.run_dir),\n",
        "            name=f\"round_train\",\n",
        "            exist_ok=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        # Compute client update (delta = new - old)\n",
        "        new_sd = self.get_state_dict()\n",
        "        delta = state_dict_difference(new_sd, global_sd)\n",
        "\n",
        "        # Optionally apply client-level DP\n",
        "        if self.dp_enabled:\n",
        "            delta = clip_and_add_noise(delta, clip_norm=self.dp_clip, noise_multiplier=self.dp_noise_mult)\n",
        "\n",
        "        # Gather some metrics (best-effort; Ultralytics APIs vary)\n",
        "        metrics = {}\n",
        "        try:\n",
        "            metrics[\"train_epochs\"] = self.local_epochs\n",
        "            metrics[\"train_batches\"] = getattr(results, \"trainer\", None) and getattr(results.trainer, \"nb\", None)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return delta, metrics\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def local_val_map(self, data_yaml: str = None) -> dict:\n",
        "        res = self.model.val(\n",
        "            data=data_yaml or self.data_yaml,\n",
        "            imgsz=self.imgsz,\n",
        "            device=0 if self.device.type == \"cuda\" else \"cpu\",\n",
        "            split=\"val\",\n",
        "            verbose=False,\n",
        "        )\n",
        "        d = getattr(res, \"results_dict\", {}) or {}\n",
        "        return {\n",
        "            \"precision\": float(d.get(\"metrics/precision(B)\", 0.0)),\n",
        "            \"recall\":    float(d.get(\"metrics/recall(B)\",    0.0)),\n",
        "            \"map50\":     float(d.get(\"metrics/mAP50(B)\",     0.0)),\n",
        "            \"map50_95\":  float(d.get(\"metrics/mAP50-95(B)\",  0.0)),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "CPy-uwpqX59z"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Federated Trainer\n",
        "# -----------------------------\n",
        "class FederatedTrainer:\n",
        "    def __init__(self, clients: List[FLClient], device: torch.device):\n",
        "        self.clients = clients\n",
        "        self.device = device\n",
        "        # Initialize global state from client 0's architecture\n",
        "        self.global_model = load_yolo_for_pretrained(clients[0].weights_path, device)\n",
        "        align_model_head_to_yaml(self.global_model, clients[0].data_yaml, device, imgsz=clients[0].imgsz)\n",
        "\n",
        "        self.global_sd = copy_state_dict(self.global_model.model.state_dict(), device)\n",
        "        \n",
        "\n",
        "    def broadcast(self):\n",
        "        for c in self.clients:\n",
        "            c.set_state_dict(self.global_sd)\n",
        "\n",
        "    def aggregate(self, deltas: List[Dict[str, torch.Tensor]]):\n",
        "        # FedAvg: average the deltas, then add to global\n",
        "        mean_delta = average_state_dicts(deltas)\n",
        "        self.global_sd = state_dict_add(self.global_sd, mean_delta)\n",
        "        # HEAD tolerances\n",
        "        self.global_model.model.load_state_dict(self.global_sd, strict=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_global_on_clients(self) -> dict:\n",
        "        Ps, Rs, maps_50, maps_50_95 = [], [], [], []\n",
        "        for c in self.clients:\n",
        "            c.set_state_dict(self.global_sd)\n",
        "            m = c.local_val_map()\n",
        "            Ps.append(m[\"precision\"]); Rs.append(m[\"recall\"])\n",
        "            maps_50.append(m[\"map50\"]); maps_50_95.append(m[\"map50_95\"])\n",
        "        return {\n",
        "            \"avg_precision\": float(np.mean(Ps)) if Ps else 0.0,\n",
        "            \"avg_recall\":    float(np.mean(Rs)) if Rs else 0.0,\n",
        "            \"avg_map50\":     float(np.mean(maps_50)) if maps_50 else 0.0,\n",
        "            \"avg_map50_95\":  float(np.mean(maps_50_95)) if maps_50_95 else 0.0,\n",
        "        }\n",
        "\n",
        "    def run(self, rounds: int = 5) -> List[dict]:\n",
        "        history = []\n",
        "        for r in range(1, rounds + 1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            print(f\"\\n=== Federated Round {r}/{rounds} ===\")\n",
        "\n",
        "            deltas = []\n",
        "            client_metrics = []\n",
        "            for c in self.clients:\n",
        "                delta, m = c.local_train_and_get_update(self.global_sd)\n",
        "                deltas.append(delta)\n",
        "                client_metrics.append(m)\n",
        "\n",
        "            # Aggregate\n",
        "            self.aggregate(deltas)\n",
        "\n",
        "            # Eval global\n",
        "            metrics = self.evaluate_global_on_clients()\n",
        "            metrics[\"round\"] = r\n",
        "            metrics[\"seconds\"] = round(time.time() - t0, 2)\n",
        "            history.append(metrics)\n",
        "            print(f\"[Round {r}] avg_mAP50={metrics['avg_map50']:.4f}, \"\n",
        "                  f\"avg_mAP50-95={metrics['avg_map50_95']:.4f}, time={metrics['seconds']}s\")\n",
        "\n",
        "        # Save final global weights (both raw sd and full Ultralytics checkpoint)\n",
        "        sd_path = Path(\"federated_best_sd.pt\")\n",
        "        torch.save(self.global_sd, sd_path)\n",
        "\n",
        "        full_ckpt_path = Path(\"federated_best_full.pt\")\n",
        "        try:\n",
        "            model_cpu = copy.deepcopy(self.global_model.model).float().cpu()\n",
        "            model_cpu.load_state_dict(self.global_sd, strict=False)\n",
        "            torch.save({\"model\": model_cpu}, full_ckpt_path)\n",
        "            print(f\"\\n[Done] Saved raw state_dict -> {sd_path.resolve()}\")\n",
        "            print(f\"[Done] Saved full Ultralytics checkpoint -> {full_ckpt_path.resolve()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Warn] Could not save full Ultralytics checkpoint: {e}. \"\n",
        "                  f\"Saved state_dict only -> {sd_path.resolve()}\")\n",
        "\n",
        "        return history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Vxetxk9MX-l1"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def censor_image(input_path: str, output_path: str, weights_path: str, conf: float = 0.25,\n",
        "                 device: torch.device = torch.device(\"cpu\")):\n",
        "    import cv2\n",
        "    import imageio.v3 as iio  # local import to ensure availability even if top import is removed\n",
        "\n",
        "    # Try to load as full Ultralytics checkpoint first\n",
        "    try:\n",
        "        model = YOLO(weights_path)\n",
        "        model.model.to(device)\n",
        "    except Exception:\n",
        "        # Fallback: treat as raw state_dict; load into a base arch\n",
        "        base = YOLO(\"yolov8n.pt\")\n",
        "        sd = torch.load(weights_path, map_location=\"cpu\")\n",
        "        if isinstance(sd, dict) and \"model\" in sd and hasattr(sd[\"model\"], \"state_dict\"):\n",
        "            model = YOLO(weights_path)\n",
        "        else:\n",
        "            base.model.load_state_dict(sd, strict=False)\n",
        "            model = base\n",
        "        model.model.to(device)\n",
        "\n",
        "    res = model.predict(\n",
        "        source=input_path,\n",
        "        conf=conf,\n",
        "        verbose=False,\n",
        "        device=0 if device.type == \"cuda\" else \"cpu\"\n",
        "    )[0]\n",
        "\n",
        "    img = cv2.imread(input_path)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {input_path}\")\n",
        "\n",
        "    boxes = res.boxes.xyxy.detach().cpu().numpy().astype(int)\n",
        "    for (x1, y1, x2, y2) in boxes:\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(img.shape[1] - 1, x2), min(img.shape[0] - 1, y2)\n",
        "        img[y1:y2, x1:x2] = 0  # solid black\n",
        "\n",
        "    iio.imwrite(output_path, img)\n",
        "    print(f\"[Censor] Wrote: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "NPX-yC01YFfK"
      },
      "outputs": [],
      "source": [
        "def parse_args(argv=None):\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Federated YOLOv8 (official weights) with optional client-level DP\",\n",
        "        allow_abbrev=False,\n",
        "    )\n",
        "    # Data/model\n",
        "    parser.add_argument(\n",
        "        \"--weights\",\n",
        "        type=str,\n",
        "        default=\"yolov8n.pt\",\n",
        "        help=\"Path or name of Ultralytics YOLOv8 checkpoint (e.g., yolov8n.pt, yolov8s.pt).\",\n",
        "    )\n",
        "    parser.add_argument(\"--client-yamls\", type=str, nargs=\"+\", default=[],\n",
        "                        help=\"Client dataset YAMLs; if empty, auto-discovers ./data/faces_clients_small/client_*.yaml\")\n",
        "    parser.add_argument(\"--imgsz\", type=int, default=640)\n",
        "    parser.add_argument(\"--batch\", type=int, default=16)\n",
        "\n",
        "    # FL\n",
        "    parser.add_argument(\"--rounds\", type=int, default=10, help=\"Federated rounds (epoch cycles).\")\n",
        "    parser.add_argument(\"--local-epochs\", type=int, default=1, help=\"Local epochs per client per round.\")\n",
        "\n",
        "    # DP\n",
        "    parser.add_argument(\"--dp\", action=\"store_true\", help=\"Enable client-level DP (clip + Gaussian noise on client update).\")\n",
        "    parser.add_argument(\"--dp-clip\", type=float, default=1.0, help=\"Client update L2 clip norm.\")\n",
        "    parser.add_argument(\"--dp-noise-mult\", type=float, default=0.5, help=\"Noise multiplier; noise std = sigma * clip_norm.\")\n",
        "\n",
        "    # Misc\n",
        "    parser.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\", \"cpu\", \"cuda\"])\n",
        "    parser.add_argument(\"--seed\", type=int, default=1337)\n",
        "    parser.add_argument(\"--workdir\", type=str, default=\"./fl_runs\", help=\"Directory for client training outputs.\")\n",
        "    parser.add_argument(\"--freeze-backbone\", action=\"store_true\")\n",
        "\n",
        "    # Optional post-training demo\n",
        "    parser.add_argument(\"--demo-image\", type=str, default=\"\",\n",
        "                        help=\"If provided, runs censoring on this image using final weights.\")\n",
        "    parser.add_argument(\"--demo-output\", type=str, default=\"output_censored.jpg\")\n",
        "    parser.add_argument(\"--demo-conf\", type=float, default=0.25)\n",
        "\n",
        "    # Notebook-safe: ignore unknown args like \"-f <kernel.json>\"\n",
        "    args, _ = parser.parse_known_args(argv)\n",
        "    return args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "DeYQ4mA1Yjuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Auto-discovered 1 client yamls under data/faces_clients_small\n",
            "    data/faces_clients_small/client_00.yaml\n",
            "client 0 task= detect nc= 80 names_len= 80\n",
            "Ultralytics 8.3.189 🚀 Python-3.13.7 torch-2.8.0 CPU (Apple M1 Pro)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/faces_clients_small/client_00.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=0, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/Yong/Desktop/NUS/TikTokTechJam2025/runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=81\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    906541  ultralytics.nn.modules.head.Detect           [81, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,166,077 parameters, 3,166,061 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 623.9±217.7 MB/s, size: 208.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/Yong/Desktop/NUS/TikTokTechJam2025/data/faces_clients_small/client_00/labels/train... 2318 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2318/2318 3661.2it/s 0.6s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/Yong/Desktop/NUS/TikTokTechJam2025/data/faces_clients_small/client_00/images/train/wider_7207.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/Yong/Desktop/NUS/TikTokTechJam2025/data/faces_clients_small/client_00/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 749.4±118.4 MB/s, size: 174.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/Yong/Desktop/NUS/TikTokTechJam2025/data/faces_clients_small/client_00/labels/val... 258 images, 1 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 258/258 3674.1it/s 0.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/Yong/Desktop/NUS/TikTokTechJam2025/data/faces_clients_small/client_00/labels/val.cache\n",
            "Plotting labels to /Users/Yong/Desktop/NUS/TikTokTechJam2025/runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000118, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/Yong/Desktop/NUS/TikTokTechJam2025/runs/detect/train4\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100         0G      3.172      6.265      2.139        214        640:  22% ━━╸───────── 32/145 0.16it/s 3:53\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh[\u001b[33m'\u001b[39m\u001b[33mround\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mh[\u001b[33m'\u001b[39m\u001b[33mavg_map50\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mh[\u001b[33m'\u001b[39m\u001b[33mavg_map50_95\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(argv)\u001b[39m\n\u001b[32m     49\u001b[39m     clients.append(c)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Federated training (FedAvg)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m trainer = \u001b[43mFederatedTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m history = trainer.run(rounds=args.rounds)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Optional demo: censor faces with final aggregated weights\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mFederatedTrainer.__init__\u001b[39m\u001b[34m(self, clients, device)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize global state from client 0's architecture\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.global_model = load_yolo_for_pretrained(clients[\u001b[32m0\u001b[39m].weights_path, device)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43malign_model_head_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.global_sd = copy_state_dict(\u001b[38;5;28mself\u001b[39m.global_model.model.state_dict(), device)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36malign_model_head_to_yaml\u001b[39m\u001b[34m(model, data_yaml, device, imgsz)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34malign_model_head_to_yaml\u001b[39m(model: \u001b[33m\"\u001b[39m\u001b[33mYOLO\u001b[39m\u001b[33m\"\u001b[39m, data_yaml: \u001b[38;5;28mstr\u001b[39m, device: torch.device, imgsz: \u001b[38;5;28mint\u001b[39m = \u001b[32m640\u001b[39m):\n\u001b[32m    101\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    Use Ultralytics' Trainer pipeline to rebuild the detection head so that 'nc'\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    matches the given data.yaml. epochs=0 avoids actual training but performs\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    model/data setup.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# <- no training; just builds according to data.yaml\u001b[39;49;00m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     model.model.to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/ultralytics/engine/model.py:799\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    798\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/ultralytics/engine/trainer.py:229\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    226\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/ultralytics/engine/trainer.py:415\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28mself\u001b[39m.tloss = (\n\u001b[32m    411\u001b[39m         (\u001b[38;5;28mself\u001b[39m.tloss * i + \u001b[38;5;28mself\u001b[39m.loss_items) / (i + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss_items\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NUS/TikTokTechJam2025/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def main(argv=None):\n",
        "    args = parse_args(argv)\n",
        "\n",
        "    # Auto-discover client yamls if not provided\n",
        "    if not args.client_yamls:\n",
        "        client_root = Path(\"./data/faces_clients_small\")  # adjust if needed\n",
        "        args.client_yamls = sorted(str(p) for p in client_root.glob(\"client_*.yaml\"))\n",
        "        print(f\"[INFO] Auto-discovered {len(args.client_yamls)} client yamls under {client_root}\")\n",
        "        for y in args.client_yamls:\n",
        "            print(\"   \", y)\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Device\n",
        "    if args.device == \"auto\":\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(args.device)\n",
        "\n",
        "    # Weights\n",
        "    weights_path = ensure_weights(args.weights)\n",
        "\n",
        "    # Build clients\n",
        "    workdir = Path(args.workdir)\n",
        "    workdir.mkdir(parents=True, exist_ok=True)\n",
        "    client_yamls = args.client_yamls\n",
        "    n_clients = len(client_yamls)\n",
        "    if n_clients == 0:\n",
        "        raise RuntimeError(\n",
        "            \"No client YAMLs found. Set --client-yamls or place client_*.yaml under ./data/faces_clients_smallest\"\n",
        "        )\n",
        "\n",
        "    clients: List[FLClient] = []\n",
        "    for i in range(n_clients):\n",
        "        c = FLClient(\n",
        "            client_id=i,\n",
        "            device=device,\n",
        "            weights_path=weights_path,\n",
        "            data_yaml=client_yamls[i],\n",
        "            workdir=workdir,\n",
        "            imgsz=args.imgsz,\n",
        "            local_epochs=args.local_epochs,\n",
        "            batch=args.batch,\n",
        "            dp_enabled=args.dp,\n",
        "            dp_clip=args.dp_clip,\n",
        "            dp_noise_mult=args.dp_noise_mult,\n",
        "            freeze_backbone=args.freeze_backbone,\n",
        "        )\n",
        "        clients.append(c)\n",
        "\n",
        "    # Federated training (FedAvg)\n",
        "    trainer = FederatedTrainer(clients, device)\n",
        "    history = trainer.run(rounds=args.rounds)\n",
        "\n",
        "    # Optional demo: censor faces with final aggregated weights\n",
        "    if args.demo_image:\n",
        "        # Prefer full Ultralytics checkpoint for predict/val\n",
        "        final_weights = \"federated_best_full.pt\"\n",
        "        if not Path(final_weights).exists():\n",
        "            # Fallback to raw sd if full ckpt isn't present\n",
        "            final_weights = \"federated_best_sd.pt\"\n",
        "        censor_image(\n",
        "            input_path=args.demo_image,\n",
        "            output_path=args.demo_output,\n",
        "            weights_path=final_weights,\n",
        "            conf=args.demo_conf,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\nRound\\tavg_mAP50\\tavg_mAP50_95\")\n",
        "    for h in history:\n",
        "        print(f\"{h['round']}\\t{h['avg_map50']:.4f}\\t\\t{h['avg_map50_95']:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
